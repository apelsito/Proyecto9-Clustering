# Proyecto 9: 

## ğŸ¯ Objetivo



## Estructura del Proyecto ğŸ—‚ï¸

```bash
Proyecto9-Clustering/
â”œâ”€â”€ datos/                      # Archivos de datos CSV y PKL para el proyecto.
â”‚   â”œâ”€â”€ 01_clustering/          # Archivos PKL de los datos para realizar los clusters.
â”‚   â”œâ”€â”€ 02_regresiones/         # Archivos PKL de los datos para realizar las regresiones.
â”‚   â”œâ”€â”€ 03_encoders/            # Archivos PKL de los modelos, encoders.. utilizados en los modelos.
â”‚
â”œâ”€â”€ jupyter_notebooks/          # Notebooks de Jupyter con los modelos probados.
â”‚   â”œâ”€â”€ Modelo X/               # Carpeta del modelo
â”‚   â”‚   â”œâ”€â”€ 01_Clustering/      # Carpeta con lo realizado para generar los clusters
â”‚   â”‚   â”œâ”€â”€ 02_Regresion_Cluster_1/ # Modelos Predictivos con el Cluster 1
â”‚   â”‚   â”œâ”€â”€ 03_Regresion_Cluster_2/ # Modelos Predictivos con el Cluster 2
â”‚ 
â”œâ”€â”€ src/                        # Archivos .py para funciones auxiliares del proyecto.
â”‚
â””â”€â”€ README.md                   # DescripciÃ³n del proyecto, instrucciones de instalaciÃ³n y uso.
```

# InstalaciÃ³n y Requisitos ğŸ› ï¸

## Requisitos

Para ejecutar este proyecto, asegÃºrate de tener instalado lo siguiente:

- **Python 3.x** ğŸ
- **Jupyter Notebook** ğŸ““ para ejecutar y visualizar los anÃ¡lisis de datos
- **Bibliotecas de Python**:
    - [pandas](https://pandas.pydata.org/docs/) para manipulaciÃ³n y anÃ¡lisis de datos ğŸ§¹
    - [numpy](https://numpy.org/doc/stable/) para cÃ¡lculos numÃ©ricos y manejo de matrices ğŸ”¢
    - [matplotlib](https://matplotlib.org/stable/index.html) para crear grÃ¡ficos bÃ¡sicos ğŸ“Š
    - [seaborn](https://seaborn.pydata.org/) para visualizaciones estadÃ­sticas avanzadas ğŸ“ˆ
    - [tqdm](https://tqdm.github.io/) para mostrar barras de progreso en procesos largos â³
    - [xgboost](https://xgboost.readthedocs.io/) para la implementaciÃ³n de modelos basados en Gradient Boosting ğŸŒŸ
    - [scikit-learn](https://scikit-learn.org/stable/) para modelado predictivo y preprocesamiento, incluyendo:
        - `LinearRegression`, `DecisionTreeRegressor`, `RandomForestRegressor`, `GradientBoostingRegressor`, y `XGBRegressor` para tareas de regresiÃ³n
        - `train_test_split`, `GridSearchCV`, `KFold`, `LeaveOneOut` y `cross_val_score` para particiÃ³n de datos y validaciÃ³n de modelos
        - `StandardScaler` para el escalado de variables
        - MÃ©tricas como `r2_score`, `mean_squared_error`, `mean_absolute_error` para evaluar los modelos
    - [pickle](https://docs.python.org/3/library/pickle.html) para serializar y cargar modelos y objetos ğŸ› ï¸

## ConfiguraciÃ³n Adicional

- Configura `pd.options.display.float_format` para un formato mÃ¡s claro en los valores flotantes.
- AÃ±ade rutas personalizadas al sistema usando `sys.path.append` para facilitar el acceso a los mÃ³dulos personalizados del proyecto.

## InstalaciÃ³n ğŸ› ï¸

1. Clona este repositorio para visualizarlo en vscode:
```bash
git clone https://github.com/apelsito/Proyecto9-Clustering.git
cd Proyecto9-Clustering
```

# Resumen de lo realizado en el Modelo final: 
### Clustering
- En el anÃ¡lisis inicial elimino las siguientes columnas
    - ['Row ID','Order ID','Order Date','Ship Date','Customer ID','Customer Name','City','State','Country','Postal Code','Product Name']
- No voy a tocar los outliers en esta fase, considero que son datos de ventas realistas que ahora mismo buscamos agrupar en clusters
- Volvemos categorÃ­a las siguientes columnas
    - ["Ship Mode","Segment","Market","Region","Category","Sub-Category","Order Priority"]

- Realizamos Frequency Encoding a todas las columnas salvo a "Product ID"

- Utilizando Robust Scaler, aplicamos feature scaling

- Ponemos como Ã­ndice la columna "Product ID"

- Realizamos un Silhoutte Score Elbow para realizar el Kmeans

- Realizamos Kmeans dividiendo en 2 grupos

- Utilizando el "Product ID" realizamos un left merge al Dataframe original de la columna "clusters_kmeans"

# Modelos de PredicciÃ³n
Dividido por Market son los siguientes:
### Cluster 1
- US
- APAC
- EU
- LATAM
### Cluster 2
- Africa
- EMEA
- Canada
### Fase 1: EDA y Ajuste de Datos
- Se eliminan las Columnas ['Row ID','Order ID','Order Date','Ship Date','Customer ID','Customer Name','City','State','Country','Postal Code','Product ID','Product Name']

- Se elimina "Sales" Tras ver las correlaciÃ³n

- Se convierten a tipo "category" las columnas de tipo "Object"
### Fase 2: Encoding
- Se utiliza Kruskal para medir el orden de las columnas

- Se realiza Target Encoding con las columnas ordinales

- Se realiza OneHot Encoding con las columnas nominales
### Fase 3: Feature Scaling
- Se utiliza Robust Scaler para normalizar los valores
### Fase 4: GestiÃ³n Outliers
- Se analizan pero no se gestionan en este modelo
### Fase 5: Modelos Predictivos
- Se realiza Decision Tree 

- Se realiza XGBoost

# Resultados del Mejor Modelo (Modelo 1)
## Cluster 1
Mejor Modelo: XGBoost
### MÃ©tricas
![MÃ©tricas](src/01_img/metricas_cluster1.png)
### Importancia
![Importancia](src/01_img/importancia_cluster1.png)
### Shap Plots
![Shap Plots](src/01_img/shap_cluster1.png)
## Cluster 2
Mejor Modelo: XGBoost
### MÃ©tricas
![MÃ©tricas](src/01_img/metricas_cluster2.png)
### Importancia
![Importancia](src/01_img/importancia_cluster2.png)
### Shap Plots
![Shap Plots](src/01_img/shap_cluster2.png)

# Conclusiones:
A partir de los resultados obtenidos podemos responder a las siguientes preguntas:

### Â¿CÃ³mo podemos agrupar a los clientes o productos de manera significativa?

Los productos los podemos agrupar por mercados en dos grupos, tenemos 4 mercados que tienen mÃ¡s compras que otros 3, por lo que los podemos separar en 2 grupos que contienen los siguientes mercados:
- Grupo 1: 
    - US
    - APAC
    - EU
    - LATAM
- Grupo 2:
    - Africa
    - EMEA
    - Canada

De esta forma mantenemos los datos equilibrados a la hora de realizar el anÃ¡lisis del profit, pero no generamos un modelo con datos demasiados disparejos que podrÃ­an dificultar las predicciones.
### Â¿QuÃ© factores son mÃ¡s relevantes para predecir el beneficio o las ventas dentro de cada grupo?
Observando ambas GrÃ¡ficas de Importancia:
-** Descuento (Discount)**: Es el factor mÃ¡s influyente, indicando que la cantidad de descuento otorgada tiene un impacto significativo en las predicciones.
- **Costo de EnvÃ­o (Shipping Cost)**: Influye de manera importante en las predicciones. Los costos de envÃ­o parecen estar correlacionados con el beneficio.
- **Cantidad (Quantity)**: La cantidad de productos vendidos tiene una relevancia considerable en las predicciones.

### Â¿CÃ³mo podemos utilizar estos insights para tomar decisiones estratÃ©gicas?

Analizando las subcategorÃ­as del cluster 1 observamos que:
![Cositas](src/01_img/cluster1_relacionProfit.png)
- Las Mesas (Tables) Generan pÃ©rdidas, habrÃ­a que ver la razÃ³n, si es que se han dado demasiados descuentos, o que no se calcula correctamente el coste de envÃ­o

AdemÃ¡s observando el resto de mÃ©tricas podrÃ­amos aplicar los siguiente:
- PolÃ­tica de Descuentos: DiseÃ±ar descuentos personalizados segÃºn el producto, el cliente y el mercado para maximizar el beneficio sin reducir mÃ¡rgenes innecesariamente.

- Nuevas Estrategias de EnvÃ­o: Analizar opciones logÃ­sticas para reducir costos, especialmente en mercados sensibles al precio del envÃ­o.

- AdaptaciÃ³n al Mercado: Dependiendo del Mercado deberemos aplicar polÃ­ticas adaptadas

# Contribuciones ğŸ¤

Las contribuciones a este proyecto son muy bienvenidas. Si tienes alguna sugerencia, mejora o correcciÃ³n, no dudes en ponerte en contacto o enviar tus ideas.

Cualquier tipo de contribuciÃ³n, ya sea en cÃ³digo, documentaciÃ³n o feedback, serÃ¡ valorada. Â¡Gracias por tu ayuda y colaboraciÃ³n!

# Autores y Agradecimientos âœï¸

## Autor âœ’ï¸
**Gonzalo RuipÃ©rez Ojea** - [@apelsito](https://github.com/apelsito) en github

## Agradecimientos â¤ï¸
Quiero expresar mi agradecimiento a **Hackio** y su equipo por brindarme la capacidad y las herramientas necesarias para realizar este proyecto con solo una semana de formaciÃ³n. Su apoyo ha sido clave para lograr este trabajo.
